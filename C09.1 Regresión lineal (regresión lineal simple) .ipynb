{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión lineal simple\n",
    "\n",
    "El problema de la regresión consiste en hallar la mejor relación funcional entre dos variables $X$ e $Y$. Más concretamente, dada una muestra de los variables $X$, $Y$, $\\{(x_i,y_i)_{i=1}^n\\}$, queremos estudiar cómo depende el valor de $Y$ en función de $X$.\n",
    "\n",
    "La variable aleatoria $Y$ es la variable dependiente o de respuesta.\n",
    "\n",
    "La variable (no necesariamente aleatoria) $X$ es la variable de control, independiente o de regresión. Pensemos por ejemplo, en un experimento donde la variable $X$ es la que controla el experimentados y la variable $Y$ es el valor que se obtiene del experimento. \n",
    "\n",
    "Si la relación funcional una recta, $Y=\\beta_0+\\beta_1X$, la regresión se denomina regresión lineal.\n",
    "\n",
    "En la regresión lineal, se hace la suposición siguiente: $\\mu_{Y|x}=\\beta_0+\\beta_1x$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los estimadores de los parámetros $\\beta_0$ y $\\beta_1$ se llamarán $b_0$ y $b_1$, respectivamente, y se obtienen a partir de la muestra. Una vez halladas las estimaciones, obtendremos la recta de regresión para nuestra muestra: $$\\widehat{y}=b_0+b_1x,$$ que dado un valor $x_0$ de $X$, estimará el valor $\\widehat{y_0}=b_0+b_1x_0$ de $Y$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mínimos cuadrados\n",
    "\n",
    "Vamos a explicar el método para hallar las estimaciones $b_0$ y $b_1$. Dicho métrodo se conoce como **mínimos cuadrados**.\n",
    "\n",
    "Dada una observación cualquiera de la muestra, $(x_i,y_i)$, podremos separar la componente $y_i$ como la suma de su valor predicho por el modelo y el error cometido: $$y_i=\\beta_0+\\beta_1x_i+\\epsilon_i,\\,\\mbox{de donde }\\epsilon_i=y_i-(\\beta_0+\\beta_1x_i).$$\n",
    "\n",
    "Llamamos **error cuadrático teórico** de este modelo a la suma al cuadrado de todos los errores cometidos por los valores de la muestra: $$SS_E=\\sum_{i=1}^n\\epsilon_i^2=\\sum_{i=1}^n(y_i-(\\beta_0+\\beta_1x_i))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresión lineal por mínimos cuadrados consiste en hallar los estimadores $b_0$ y $b_1$ que minimicen el error cuadrático teórico. Es sencillo demostrar que los valores que optimizan son \n",
    "\n",
    "$$\\begin{array}{ccl}b_0&=&\\frac{\\displaystyle n\\displaystyle\\sum_{i=1}^nx_iy_i-\\sum_{i=1}^nx_i\\sum_{i=1}^ny_i}{\\displaystyle n\\displaystyle\\sum_{i=1}^nx_i^2-\\left(\\displaystyle\\sum_{i=1}^nx_i\\right)^2}\\\\&&\\\\b_1&=&\\frac{\\displaystyle n\\displaystyle\\sum_{i=1}^ny_i-b_1\\sum_{i=1}^nx_i}{\\displaystyle n}\\\\\\end{array}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos uso ahora de las medias, varianzas y covarianzas muestrales. En particular. \n",
    "\n",
    "$$\\tilde{S}_X^2=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\overline{x})^2=\\frac{n}{n-1}\\left(\\frac{1}{n}\\sum_{i=1}^nx_i^2-\\overline{x}^2\\right)$$\n",
    "\n",
    "$$\\tilde{S}_Y^2=\\frac{1}{n-1}\\sum_{i=1}^n(y_i-\\overline{y})^2=\\frac{n}{n-1}\\left(\\frac{1}{n}\\sum_{i=1}^ny_i^2-\\overline{x}^2\\right)$$\n",
    "\n",
    "$$\\tilde{S}_{XY}=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\overline{x})(y_i-\\overline{y})=\\frac{n}{n-1}\\left(\\frac{1}{n}\\sum_{i=1}^nx_iy_i-\\overline{x}\\,\\overline{y}\\right)$$\n",
    "\n",
    "son las varianzas y covarianza muestrales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teorema.**\n",
    "\n",
    "Los estimadores $b_0$ y $b_1$ del método de los mínimos cuadrados se reescriben como $$b_1=\\frac{\\tilde{S}_{XY}}{\\tilde{S}_X^2}\\mbox{ y }b_0=\\overline{y}-b_1\\overline{x}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado un valor $x$ de $X$, llamaremos **valor estimado de $Y$ cuando $X=x$** a $\\widehat{y}$ de la expresión $\\widehat{y}=b_0+b_1x$. Dada una observación $(x_i,y_i)$, llamamos error de la observación, denotado por $e_i$, a la expresión $e_i=y_i-\\widehat{y_i}=y_i-(b_0+b_1x_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo.**\n",
    "\n",
    "En un experimento donde se quería estudiar la asociación entre consumo de sal y presión arterial, se asignó aleatoriamente a algunos individuos una cantidad diaria constante de sal en su dieta, y al cabo de un mes se les midió la tensión arterial media. Algunos resultados fueron los siguientes:\n",
    "\n",
    "|Sal (g)| Presión (mm de Hg)|\n",
    "|:--:|:--:|\n",
    "|1.8|100|\n",
    "|2.2|98|\n",
    "|3.5|110|\n",
    "|4.0|110|\n",
    "|4.3|112|\n",
    "|5.0|120|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función lm en R\n",
    "\n",
    "En R, podemos hacer los cálculos de $b_0$ y $b_1$ utilizando la función lm. Esta tiene la sintaxis:\n",
    "\n",
    "``lm(Y~X,datos)$coefficients``\n",
    "\n",
    "donde \n",
    "\n",
    "Y es la variable de respuesta.\n",
    "X es la variable de control.\n",
    "datos es la tabla donde se encuentra la información.\n",
    "\n",
    "Además, para observar el gráfico junto con la recta de la regresión lineal podemos usar el siguiente plot:\n",
    "\n",
    "``ggplot(data = datos) +\n",
    "  geom_point(mapping = aes(x = X,y = Y)) +\n",
    "  geom_abline(mapping = aes(intercept = lm(Y~X,datos)$coefficients[1],\n",
    "                            slope = lm(Y~X,datos)$coefficients[2]\n",
    "                            )\n",
    "             )``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propiedades de la recta de regresión.\n",
    "\n",
    "\n",
    "* La recta de regresión siempre pasa por el punto $(\\overline{x},\\overline{y})$ de la muestra. Es decir $$\\overline{y}=b_0+b_1\\overline{x}.$$\n",
    "\n",
    "\n",
    "* La media de los valores estimados a partir de la recta de regresión es igual a la media de los valores observados. Es decifr $$\\overline{\\widehat{y}}=\\overline{y}.$$\n",
    "\n",
    "\n",
    "* Los errores tienen media 0:\n",
    "$$\\overline{e}=\\frac{1}{n}\\sum_{i=1}^ne_i=0$$\n",
    "\n",
    "\n",
    "Llamaremos **suma de cuadrados de los errores** a $$SS_E=\\sum_{i=1}^ne_i^2.$$\n",
    "\n",
    "Usando que los errores tienen media cero, su varianza será:\n",
    "$$S^2_e=\\frac{SS_E}{n}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las variables aleatorias $E_{x_i}$ como $E_{x_i}=y_i-(b_0+b_1x_i)$, donde $(x_i,y_i)$ es un valor de la muestra y $b_0,b_1$ son loes estimadores obtenidos por el método de los mínimos cuadrados. Entonces:\n",
    "\n",
    "**Teorema.** Si las variables aleatorias $E_{x_i}$ tienen todas media 0 y la misma varianza $\\sigma^2_E$ y, dos a dos, tienen covarianza 0, entonces $b_0$ y $b_1$ son los estimadores lineales insesgados óptimos de $\\beta_0$ y $\\beta_1$, y un estimador insesgado de $\\sigma_E^2$ es $S^2=\\frac{SS_E}{n-2}.$\n",
    "\n",
    "Si además las variables $E_{x_i}$ son normales, entonces $b_0$ y $b_1$ son los estimadores de máxima verosimilitud de $\\beta_0$ y $\\beta_1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coeficiente de determinación\n",
    "\n",
    "Llegados a este punto, nos preguntamos lo efectiva que es la recta de regresión. Es decir, cómo medir si la aproximación hallada a la nube de puntos ha sido suficientemente buena.\n",
    "\n",
    "Una forma de realizar dicha medición es através del coeficiente de determinación $R^2$ que estima cuánta variabilidad de los valores $y_i$ heredan los valores estimados $\\widehat{y_i}$.\n",
    "\n",
    "Para ver su definición, necesitamos introducir las variabilidades siguientes:\n",
    "\n",
    "* **Variabilidad total** o suma total de los cuadrados: $$SS_T=\\sum_{i=1}^n(y_i-\\overline{y})^2=(n-1)\\tilde{S}^2_Y$$\n",
    "\n",
    "\n",
    "* **Variabilidad de la regresión** o suma de los cuadrados de la regresión: $$SS_R=\\sum_{i=1}^n(\\widehat{y}_i-\\overline{y})^2=(n-1)\\tilde{S}^2_\\widehat{Y}$$\n",
    "\n",
    "\n",
    "* **Variabilidad del error** o suma de los cuadrados del error: $$SS_E=\\sum_{i=1}^n(y_i-\\widehat{y}_i)^2=(n-1)\\tilde{S}_e^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teorema.** En una regresión lineal usando el método de los mínimos cuadrados se cumple la siguiente relación entre las variabilidades: $SS_T=SS_R+SS_E$, o equivalentemente $$\\tilde{S}_Y^2=\\tilde{S}_\\widehat{Y}^2+\\tilde{S}_e^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, cuantas más \"próximas\" estén las variabilidades $SS_T$ y $SS_R$, más efectiva habrá sido la regresión, ya que habrá heredado mucha variabilidad de los datos y la variabilidad del error será pequeña.\n",
    "\n",
    "Se define el **coeficiente de determinación $R^2$** en la regresión por el método de los mínimos cuadrados como $$R^2=\\frac{SS_R}{SS_T}$$\n",
    "\n",
    "$R^2$ es una cantidad entre 0 y 1. Cuanto más próximo a 1 esté dicho coeficiente, más precisa será la recta de regresión.\n",
    "\n",
    "\n",
    "Se define el **coefienciente de correlación lineal $r_{xy}$** como $$r_{xy}=\\frac{\\tilde{s}_{xy}}{\\tilde{s}_{x}\\tilde{s}_{y}}$$\n",
    "\n",
    "\n",
    "Entonces $$R^2=r^2_{xy}$$ \n",
    "\n",
    "En R, el coeficiente de determinación se halla con ``summary(lm(y~x,datos))$r.squared``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El cuarteto de Anscombe\n",
    "\n",
    "Usar solamente el coeficiente de determinación para medir la calidad de la regresión es un error. Tenemos que observar mas información para poder afirmar que la regresión obtenida es adecuada y se ajusta a nuestros datos.\n",
    "\n",
    "En R existe una tabla llamada anscombe que pone de manifiesto este hecho. Para mandarla a llamar, basta con hacer data(anscombe).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio.** Calcular las regresiones lineales posibles de la tabla de Anscombe y su coeficiente de determinación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervalos de confianza\n",
    "\n",
    "Para poder hallar los intervalos de confianza al $100\\cdot(1-\\alpha)\\%$ sobre los parámetros $\\beta_0$ y $\\beta_1$, necesitamos los supuestos siguientes:\n",
    "\n",
    "Para cada valor $x_i$ de la variable $X$, las variables aleatorias $E_{x_i}$ siguen una distribución $N(0,\\sigma^2_E)$, donde la desviación es constante. También supondremos que dados $x_i$ y $x_j$, la covarianza es nula.\n",
    "\n",
    "Bajo estas suposiciones:\n",
    "\n",
    "**Teorema.** Los errores estandar de los estimadores $b_0$ y $b_1$ son $$\\frac{\\sigma_E}{\\tilde{S}_X\\sqrt{n-1}}\\mbox{ y }\\sigma_E\\sqrt{\\frac{1}{n}+\\frac{\\overline{X}^2}{(n-1)\\tilde{S}^2_X}}$$ donde para estimar $\\sigma_E$ usamos la desviación muestral de los errores $S$.\n",
    "\n",
    "**Teorema** Las variables aleatorias $$\\frac{b_1-\\beta_1}{\\frac{S}{\\tilde{S}_X\\sqrt{n-1}}}\\mbox{ y }\\frac{b_0-\\beta_0}{S\\sqrt{\\frac{1}{n}+\\frac{\\overline{X}^2}{(n-1)\\tilde{S}^2_X}}}$$ son $t$ de Student con $n-2$ grados de libertad.\n",
    "\n",
    "Por lo tanto, los intervalos de confianza para $\\beta_0$ y $\\beta_1$ al $100(1-\\alpha)\\%$ son los siguientes:\n",
    "$$\\begin{array}{l}\\beta_1:\\,\\left(b_1-t_{n-2,1-\\frac{\\alpha}{2}}\\frac{S}{\\tilde{S}_X\\sqrt{n-1}},b_1+t_{n-2,1-\\frac{\\alpha}{2}}\\frac{S}{\\tilde{S}_X\\sqrt{n-1}}\\right)\\\\\n",
    "\\beta_0:\\,\\left(b_0-t_{n-2,1-\\frac{\\alpha}{2}}S\\sqrt{\\frac{1}{n}+\\frac{\\overline{X}^2}{(n-1)\\tilde{S}_X^2}},b_0+t_{n-2,1-\\frac{\\alpha}{2}}S\\sqrt{\\frac{1}{n}+\\frac{\\overline{X}^2}{(n-1)\\tilde{S}_X^2}}\\right)\n",
    "\\end{array}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La función lm\n",
    "\n",
    "Cuando hacemos summary(lm(...)) nos dará mucha información acerca del modelo lineal. Para los intervalos de confianza anteriores, usamos confint(lm(...),level=0.95)\n",
    "\n",
    "<img src=\"lm.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
