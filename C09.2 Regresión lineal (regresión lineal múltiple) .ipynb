{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión lineal múltiple\n",
    "\n",
    "En la regresión lineal simple, estudiamos si una variable dependiente $Y$ depende linealmente de una variable independiente o de control $X$.\n",
    "\n",
    "En la práctica, dicha situación rara vez se da, ya que la variable dependiente suele depender de mas un control. Por lo tanto, en esta sección vamos a generalizar todo el estudio que hemos hecho para la regresión lineal simple al caso en que tengamos $k$ variables de control $X_1,X_2,\\cdots,X_k$. Es decir, una variable dependiente $Y$ y $k$ variables de control (si se tienen al menos dos variables dependientes, en la práctica lo que se hace es establecer al menos dos modelos: uno por variable dependiente).\n",
    "\n",
    "Suponemos ahora que nuestro modelo es de la forma $$\\mu_{Y|x_1,x_2,...,x_k}=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdots+\\beta_kx_k.$$ \n",
    "\n",
    "Los valores $\\beta_0,\\beta_1,...,\\beta_k$ son los llamados parámetros de la regresión y se tienen que estimar a partir de una muestra las variables consideradas: $\\{(x_{i1},x_{i2},...,x_{ik},y_i)\\}_{i=1}^n$.\n",
    "\n",
    "Para que estas estimaciones se pueden realizar hay que suponer que $n>k$ ya que en caso contrario tendríamos un problema subestimado: tendríamos más parámetros que valores en la muestra.\n",
    "\n",
    "Denotamos por $\\boldsymbol{x_i}$ a los valores de las $k$ variables independientes correspondientes al $i$-ésimo elemento de la muestra. Esto es: $\\boldsymbol{x_i}=(x_{i1},x_{i2},\\cdots,x_{ik})$.\n",
    "\n",
    "Escribimos el modelo de la siguiente manera: $$Y=\\beta_0+\\beta_1X_1+...+X_k\\beta_k+E_\\boldsymbol{X_i}.$$\n",
    "\n",
    "A partir de la muestra $\\{(\\boldsymbol{x_i},y_i)\\}_{i=1}^n$ vamos a obtener estimaciones $b_0,b_1,\\cdots,b_k$ para los parámetros $\\beta_1,\\beta_2,\\cdots,\\beta_k$.\n",
    "\n",
    "Una vez obtenidas estas estimaciones, podemos definir los valores siguientes:\n",
    "\n",
    "$$\\begin{array}{l}\\widehat{y_i}=b_0+b_1x_{i1}+...+b_kx_{ik},\\\\y_i=b_0+b_1x_{i1}+...+b_kx_{ik}+e_i\\end{array}$$\n",
    "\n",
    "Para simplificar la notación, escribimos los datos de la muestra en forma matricial. En primer lugar, definimos los vectores siguientes:\n",
    "\n",
    "$$\\boldsymbol{y}=\\left(\\begin{array}{c}y_1\\\\y_2\\\\\\vdots\\\\y_n\\end{array}\\right),\\,\\boldsymbol{b}=\\left(\\begin{array}{c}b_0\\\\b_1\\\\\\vdots\\\\b_k\\end{array}\\right),\\,\\boldsymbol{\\widehat{y}}=\\left(\\begin{array}{c}\\widehat{y_1}\\\\\\widehat{y_2}\\\\\\vdots\\\\\\widehat{y_n}\\end{array}\\right),\\,\\boldsymbol{e}=\\left(\\begin{array}{c}e_1\\\\e_2\\\\\\vdots\\\\e_n\\end{array}\\right),\\,$$\n",
    "\n",
    "\n",
    "Definimos $\\boldsymbol{\\mathrm{X}}$ a partir de los datos de la muestra de las variables $\\boldsymbol{X}_i$:\n",
    "$$\\boldsymbol{\\mathrm{X}}=\\left(\\begin{array}{ccccc}1&x_{11}&x_{12}&\\cdots&x_{1k}\\\\1&x_{21}&x_{22}&\\cdots&x_{2k}\\\\\\vdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\1&x_{n1}&x_{n2}&\\cdots&x_{nk}\\end{array}\\right)$$\n",
    "\n",
    "Por lo tanto $\\boldsymbol{\\widehat{y}}=\\boldsymbol{\\mathrm{X}}\\boldsymbol{b}$ y $\\boldsymbol{y}=\\boldsymbol{\\mathrm{X}\\boldsymbol{b}}+\\boldsymbol{e}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimación de los parámetros por mínimos cuadrados.\n",
    "\n",
    "Definimos el error cuadrático $SS_E$ como $$SS_E=\\sum_{i=1}^ne_i=\\sum_{i=1}^n(y_i-\\widehat{y_i})^2=\\sum_{i=1}^n(y_i-(b_0+b_1x_{i1}+...+b_kx_{ik}))^2=\\|\\boldsymbol{y}-\\boldsymbol{\\mathrm{X}}\\boldsymbol{b}\\|^2$$\n",
    "\n",
    "Los estimadores $b_0,b_1,\\cdots,b_k$ que buscamos son los que minimicen a $SS_E$.\n",
    "\n",
    "**Teorema.** Los estimadores por el método de los mínimos cuadrados de los parámetros $\\{\\beta_i\\}$ a partir de la muestra $(\\boldsymbol{x_i},y_i)_{i=1}^n$, son los siguientes: $$\\boldsymbol{b}=(\\boldsymbol{\\mathrm{X}}^T\\boldsymbol{\\mathrm{X}})^{-1}( \\boldsymbol{\\mathrm{X}}^T\\boldsymbol{y})$$ (ver el pdf [DemostracionMS](https://github.com/scidatmath2020/Inferencia-Estadistica/blob/master/DemostracionMS.pdf) en nuestro repositorio).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular los coeficientes de la regresión en R por mínimos cuadrados, de nuevo usamos la función lm:\n",
    "\n",
    "``lm(Y~Variable_1+Variable_2+...+Variable_k,datos)``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo.**\n",
    "\n",
    "Se postula que la altura de un bebé tiene una relación lineal con su edad en días ($x_1$), su altura al nacer en cm ($x_2$), su peso en Kg al nacer $($x_3$)$ y el aumento, en tanto por ciento, de su peso actual respecto de su peso al nacer ($x_4$). \n",
    "\n",
    "El modelo es $Y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_3+\\beta_4x_4$.\n",
    "\n",
    "En una muestra de 9 niños, se obtuvieron los siguientes resultados.\n",
    "\n",
    "|y|x1|x2|x3|x4|\n",
    "|:--:|:--:|:--:|:--:|:--:|\n",
    "|57.5|78|48.2|2.75|29.5|\n",
    "|52.8|69|45.5|2.15|26.3|\n",
    "|61.3|77|46.3|4.41|32.2|\n",
    "|67|88|49|5.52|36.5|\n",
    "|53.5|67|43|3.21|27.2|\n",
    "|62.7|80|48|4.32|27.7|\n",
    "|56.2|74|48|2.31|28.3|\n",
    "|68.5|94|53|4.3|30.3|\n",
    "|69.2|102|58|3.71|28.7|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo.**\n",
    "\n",
    "En un estudio sobre la población de un parásito se hizo un recuento de parásitos en 15 localizaciones con diversas condiciones ambientales. Los datos obtenidos son los siguientes:\n",
    "\n",
    "| | | | | | | | | | | | | | | | |\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|Temperatura|15|16|24|13|21|16|22|18|20|16|28|27|13|22|23|\n",
    "|Humedad    |70|65|71|64|84|86|72|84|71|75|84|79|80|76|88|\n",
    "|Recuento   |156|157| 177| 145| 197| 184| 172| 187| 157| 169| 200| 193| 167| 170| 192|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propiedades de la función de regresión.\n",
    "\n",
    "* La regresión lineal múltiple pasa por el vector de medias $(\\overline{x_1},\\overline{x_2},...,\\overline{x_k},\\overline{y})$. Es decir, $$\\overline{y}=b_0+b_1\\overline{x_1}+b_2\\overline{x_2}+\\cdots+b_k\\overline{x_k}.$$\n",
    "\n",
    "\n",
    "* La media de los valores estimados es igual a la media de los obeservados: $$\\overline{\\widehat{y}}=\\overline{y}.$$\n",
    "\n",
    "\n",
    "* Los errores $\\{e_i\\}_{i=1}^n$ tienen media 0 y varianza $$\\tilde{S}_e^2=\\frac{SS_E}{n-1}.$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coeficiente de determinación.\n",
    "\n",
    "Al igual que hicimos con la regresión lineal simple, vamos a definir el coeficiente de determinación que es una manera de medir lo efectiva que es la regresión.\n",
    "\n",
    "Introducimos las siguientes variabilidades:\n",
    "\n",
    "* **Variabilidad total** o suma total de los cuadrados: $$SS_T=\\sum_{i=1}^n(y_i-\\overline{y})^2=(n-1)\\tilde{S}^2_Y$$\n",
    "\n",
    "\n",
    "* **Variabilidad de la regresión** o suma de los cuadrados de la regresión: $$SS_R=\\sum_{i=1}^n(\\widehat{y}_i-\\overline{y})^2=(n-1)\\tilde{S}^2_\\widehat{Y}$$\n",
    "\n",
    "\n",
    "* **Variabilidad del error** o suma de los cuadrados del error: $$SS_E=\\sum_{i=1}^n(y_i-\\widehat{y}_i)^2=(n-1)\\tilde{S}_e^2$$\n",
    "\n",
    "Además nuevamente la variabilidad total se puede descomponer como la suma de la variabilidad de la regresión y la variabilidad del error.\n",
    "\n",
    "Se define el **coeficiente de determinación $R^2$** en la regresión por el método de los mínimos cuadrados como $$R^2=\\frac{SS_R}{SS_T}$$\n",
    "\n",
    "$R^2$ es una cantidad entre 0 y 1. Cuanto más próximo a 1 esté dicho coeficiente, más precisa será la recta de regresión.\n",
    "\n",
    "\n",
    "Se define el **coefienciente de correlación múltiple de $y$ respecto de $(x_1,x_2,\\cdots,x_k)$** R.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de modelos: coeficiente de determinación ajustado \n",
    "\n",
    "El coeficiente de determinación definido anteriormente aumenta si aumentamos el número de variables independientes $k$, incluso si estas aportan información redundante o poca información. Por ejemplo, si añadimos variables que son linealmente dependientes de las demás.\n",
    "\n",
    "Para evitar este problema, o para penalizar el aumento de variables independientes se usa en su lugar el **coeficiente de regresión ajustado:** $$R^2_{adj}=\\frac{MS_T-MS_E}{MS_T}$$ donde $MS_T=\\frac{SS_T}{n-1}$ y $MS_E=\\frac{SS_E}{n-k-1}$. La relación entre ambos coeficientes de determinación es $$R^2_{adj}=1-(1-R^2)\\frac{n-1}{n-k-1}$$\n",
    "\n",
    "En general, $0\\le R^2_{adj}<R^2\\le1$, por lo el coeficiente de determinación ajustado es más difícil obtener un valor cercano a 1.\n",
    "\n",
    "Para calcularlo con R basta con la instrucción `summary(lm(...))$adj.r.squared`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de modelos: AIC y BIC.\n",
    "\n",
    "Cuando comparamos dos modelos lineales múltiples para ver cuál es más adecuado que el otros, el coeficiente de regresión ajustado es uno de los métodos.\n",
    "\n",
    "Existen más métodos como son el AIC o el BIC (criterio de Akaike y criterio bayesiano).\n",
    "\n",
    "### Índice de Akaike\n",
    "\n",
    "El AIC cuantifica cuánta información de $Y$ se pierde con el modelo y cuántas variables usamos. Concretamente, se define como $$AIC=n\\ln\\left(\\frac{SS_E}{n}\\right)+2k$$ y el modelo con menor AIC es el más adecuado.\n",
    "\n",
    "En R, para calcularlo usamos ``AIC(lm(...))``.\n",
    "\n",
    "### Índice bayesiano\n",
    "\n",
    "Análogo al método de Akaike, el BIC cuantifica cuánta información de $Y$ se pierde con el modelo y cuántas variables usamos. Concretamente, se define como $$BIC=n\\ln\\left(\\frac{SS_E}{n}\\right)+k\\ln(n)$$ y el modelo con menor BIC es el más adecuado.\n",
    "\n",
    "En R, para calcularlo usamos ``BIC(lm(...))``.\n",
    "\n",
    "\n",
    "El AIC intenta seleccionar el modelo que describa más adecuadamente una variable dependiente de alta complejidad. Esto significa que el modelo real nunca está en el conjunto de modelos candidatos que se están considerando. Por el contrario, BIC intenta encontrar el modelo VERDADERO entre el conjunto de candidatos.\n",
    "\n",
    "Según el AIC, todos los modelos son aproximaciones a la realidad, y la realidad nunca tiene una baja dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervalos de confianza\n",
    "\n",
    "Vamos a calcular los intervalos de confianza para los $k+1$ parámetros del modelo: $\\beta_0,\\beta_1,\\cdots,\\beta_k$. Para ello, al igual que hicimos en la regresión lineal simple, supondremos que las variables aleatorias $E_i=E_{\\boldsymbol{x_i}}$ son incorreladas; todas son normales con media 0 y de la misma varianza $\\sigma^2_E$.\n",
    "\n",
    "Además de dar los intervalos de confianza, necesitamos conocer las propiedades sobre los estimadores de los parámetros anteriores; es decir, si son insesgados, cómo estimar la varianza común y sus errores estandar.\n",
    "\n",
    "**Teorema.** Bajo las hipótesis anteriores, los estimadores obtenidos por el método de mínimos cuadrados son máximoverosimiles e insesgados. Además $$Cov(b_0,b_1,\\cdots,b_k)=\\sigma_E^2(X^TX)^{-1},$$ donde $Cov(b_0,b_1,\\cdots,b_k)$ es la matriz de covarianzas ($Cov(b_0,b_1,\\cdots,b_k)_{ij}=Cov(b_i,b_i)$) y un estimador insesgado para la varianza $\\sigma_E^2$ es $S^2=\\frac{SS_E}{n-k-1}$\n",
    "\n",
    "**Teorema.** Bajo las hipótesis anteriores, \n",
    "\n",
    "* El error estándar de cada estimador $b_i$ vale $$\\sqrt{\\sigma_E^2(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}_{ii}}.$$ \n",
    "\n",
    "\n",
    "* La variable aleatoria $\\frac{\\beta_i-b_i}{\\sqrt{\\sigma_E^2(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}_{ii}}}$ es una $t_{(n-k-1)}$\n",
    "\n",
    "\n",
    "* Un intervalo de confianza del $100(1-\\alpha)\\%$ de confianza para $\\beta_i$ es $$\\left(b_i-t_{n-k-1,1-\\frac{\\alpha}{2}}\\sqrt{\\sigma_E^2(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}_{ii}},b_i+t_{n-k-1,1-\\frac{\\alpha}{2}}\\sqrt{\\sigma_E^2(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}_{ii}}\\right)$$\n",
    "\n",
    "En R, los intervalos de confianza anteriores se pueden hallar con ``confint(lm(...))``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mas sobre intervalos de confianza: estudio de predicciones.\n",
    "\n",
    "Fijados valores concretos de $(x_1,x_2,...,x_n)$, digamos $(x_{01},x_{02},...,x_{0k})$, podemos considerar dos parámetros más a estudiar: el valor medio de la variable aleatoria $Y|_{x_{01},x_{02},...,x_{0k}}$ y el valor estimado $y_0=b_0+b_1x_{10}+...+b_kx_{k0}$ por la regresión.\n",
    "\n",
    "**Teorema.** Sean $\\boldsymbol{x_0}=(x_{01},x_{02},...,x_{0k})$ valores concretos de las variables $(X_1,X_2,...,X_k)$. Bajo las hipótesis anteriores:\n",
    "\n",
    "* El error estándar de $\\widehat{y_0}$ como estimador del parámetro $\\mu|_{\\boldsymbol{x_0}}$ es $$S\\sqrt{\\boldsymbol{x_0^\\prime}(\\boldsymbol{\\mathrm{X}}^T\\boldsymbol{\\mathrm{X}})^{-1}\\boldsymbol{x_0}^{\\prime T}}.$$\n",
    "\n",
    "\n",
    "* El error estándar de $\\widehat{y_0}$ como estimador de $y_0$ es $$S\\sqrt{1+\\boldsymbol{x_0^\\prime}(\\boldsymbol{\\mathrm{X}}^T\\boldsymbol{\\mathrm{X}})^{-1}\\boldsymbol{x_0}^{\\prime T}},$$ donde $\\boldsymbol{x_0^\\prime}=(1,x_{01},x_{02},...,x_{0k})$.\n",
    "\n",
    "\n",
    "**Teorema.** Las variables aleatorias $$\\frac{\\mu|_{\\boldsymbol{x_0}}-\\widehat{y_0}}{S\\sqrt{\\boldsymbol{x_0^\\prime}(\\boldsymbol{\\mathrm{X}}^T\\boldsymbol{\\mathrm{X}})^{-1}\\boldsymbol{x_0}^{\\prime T}}},\\,\\,\\frac{y_0-\\widehat{y_0}}{S\\sqrt{1+\\boldsymbol{x_0^\\prime}(\\boldsymbol{\\mathrm{X}}^T\\boldsymbol{\\mathrm{X}})^{-1}\\boldsymbol{x_0}^{\\prime T}}}$$ siguen una $t_{(n-k-1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Así, los intervalos de confianza al $100(1-\\alpha)\\%$ de confianza para ambos parámetros vienen dados por $$\\left(\\mu|_{\\boldsymbol{x_0}}-t_{n-k-1,1-\\frac{\\alpha}{2}}S\\sqrt{\\boldsymbol{x_0^\\prime}(\\boldsymbol{\\mathrm{X}}^T\\boldsymbol{\\mathrm{X}})^{-1}\\boldsymbol{x_0}^{\\prime T}},\\mu|_{\\boldsymbol{x_0}}+t_{n-k-1,1-\\frac{\\alpha}{2}}S\\sqrt{\\boldsymbol{x_0^\\prime}(\\boldsymbol{\\mathrm{X}}^T\\boldsymbol{\\mathrm{X}})^{-1}\\boldsymbol{x_0}^{\\prime T}}\\right)$$ y \n",
    " \n",
    "$$\\left(\\widehat{y_0}-t_{n-k-1,1-\\frac{\\alpha}{2}}S\\sqrt{1+\\boldsymbol{x_0^\\prime}(\\boldsymbol{\\mathrm{X}}^T\\boldsymbol{\\mathrm{X}})^{-1}\\boldsymbol{x_0}^{\\prime T}},\\widehat{y_0}+t_{n-k-1,1-\\frac{\\alpha}{2}}S\\sqrt{1+\\boldsymbol{x_0^\\prime}(\\boldsymbol{\\mathrm{X}}^T\\boldsymbol{\\mathrm{X}})^{-1}\\boldsymbol{x_0}^{\\prime T}}\\right)$$\n",
    "\n",
    "En R, para hallar estos intervalos de confianza hacemos lo siguiente \n",
    "\n",
    "``newdata = data.frame(x1=x10,x2=x20,...,xk=xk0)``\n",
    "\n",
    "``predict.lm(lm(y~x1+x2+...+xk,tabla de datos),newdata,interval = \"¿A QUIÉN VAS A ESTIMAR?\",level = nivel.confianza)``\n",
    "\n",
    "donde el parámetro interval es ``confidence`` si se desea el intervalo de confianza para $\\mu|_{\\boldsymbol{x_0}}$, y es igual a ``prediction`` si se desea para el parámetro $y_0$.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
